title: "Hadoop实战"
date: 2015-04-19 09:12:01
tags: Hadoop
categories: Hadoop
---


## 课程安排

>+ 流量运营系统项目整体介绍
+ 流量运营系统项目技术架构
+ 流量运营系统项目技术要点
+ 流量运营系统项目模块开发实战

## 项目整体介绍
>+ 流量经营项目相关背景
+ 流量经营数据来源、内容（实例展示，字段介绍）
+ 流量经营数据处理流程（预处理,SCA,TAS等）
+ 数据分析结果（各阶段中间结果，最终结果）

## 项目背景——项目起因
>+ 运营商掌握了大量的用户上网行为数据
+ 用户上网行为数据丰富的商业价值 
+ 相对廉价方便的大数据处理技术使得海量数据挖掘分析成为可能
+ 互联网企业对传统电信运营商的倒逼（市场，业务，技术）

## 项目背景——营销支撑
>+ 对客户移动互联网行为进行采集，分析，发现用户关注相关内容，为开展营销提供号码支



## 项目背景——投产成果
>+ 用户响应率有三倍提升
    + 本次营销相关激励措施和前期开展的WAP PUSH营销相同；
    + 对比以往的群发响应率1%-3%，本次群发响应率效果明显，达到5.80%-10.21%，有近乎3倍的提升效果。
+ 访问用户活跃度高
    + 用户后有继续点击其他内容的行为，最高占比达到91.4%；
    + 产生二次点击行为的用户数的占比高，反映贴合用户需求的内容对用户的吸引力，用户粘性越高。

## 项目背景——项目概况
>+ 集群：3个
    + 数据采集集群（6-10节点）
    + 行为轨迹增强处理集群（20-25节点）
    + ETL、统计分析集群（35节点）
+ 数据量：每天2T左右（10亿行以上），并在不断增长
+ 项目组规模
    + 研发团队、实施团队、运维团队

## 项目所处理的数据
>+ 数据来源：
    + 数据的采集可以是从硬件设备（如网关、Gn口、分光设备）直接获取并解析
        也可以是从其它系统（如BOSS和VGOP）导入
>+ 数据类型：
    + HTTP日志/WAP日志/MMS日志/ CONN日志/DNS日志


## 项目所处理的数据
+ 数据格式及内容：
```
http日志示例：
1374609560.11   1374609560.16   1374609560.16   1374609560.16   110 5   8615038208365   460023383869133 8696420056841778    2   460 0   14615           54941   10.188.77.252   61.145.116.27   35020   80  6   cmnet   1   221.177.218.34  221.177.217.161 221.177.218.34  221.177.217.167 ad.veegao.com   http://ad.veegao.com/veegao/iris.action     Apache-HttpClient/UNAVAILABLE (java 1.4)    POST    200 593 310 4   3   0   0   4   3   0   0   0   0   http://ad.veegao.com/veegao/iris.action 5903903079251243019 5903903103500771339 5980728
```

## 项目数据处理流程

>1 数据采集:清洗,分类,合并.上传HDFS集群
2 数据处理：内容识别,用户行为轨迹增强
3 数据挖掘、统计分析
4 业务应用、BI报表展示

## 项目数据处理结果
>+ 原始日志（plain text）
+ 分类合并日志（plain text）
+ 行为轨迹增强日志（plain text）
+ 待爬清单（plain text）
+ 挖掘、分析结果入库（关系型数据表）

## 系统架构设计
>+ 系统整体架构（系统分界，子系统划分，模块结构、层次结构）
+ 主要技术选型
+ 关键子系统SCA数据处理流程
+ 关键子系统SCA主要功能模块

## 系统整体架构
![](/pic/hadoop/xm-02.png)

## 主要技术选型

>+ 数据采集：根据不同生产环境，有多种形式
+ 云存储：HDFS，事实上的大数据技术标准
+ 海量数据批处理：MAP/REDUCE
+ 爬虫系统：Nutch，技术成熟，功能齐全，文档丰富，易扩展易改造
+ 内容识别：模板匹配，XPATH，贝叶斯分类
+ 云ETL： HIVE，最通用成熟的大数据平台ETL/数据仓库工具；Python脚本

### 核心子系统SCA功能模块组成

>+ 数据采集：根据不同生产环境，有多种形式
+ 数据预处理
+ 数据上传HDFS
+ 行为轨迹增强
+ 内容识别

##项目技术要点
>+ 数据预处理（采集，分类，上传HDFS）
+ 规则分类（在mapreduce中查询关系型数据库）
+ 实例分类（在mapreduce中查询kv数据库）
+ 内容识别（爬虫，模板、语义识别）
+ 定时任务、结果推送
+ BI统计分析（实际投产脚本选样讲解）
+ 报表展现（JAVA WEB）

## 数据预处理
>+ 数据采集：FTP/SHELL脚本/flume/activemq/socket/kafka
+ 数据预处理：JAVA （多线程，IO操作）
+ 数据上传HDFS：HDFS API
![](/pic/hadoop/xm-03.png)

##用户行为轨迹增强
###规则分类
>+ 规则库设计：分类体系，MYSQL关系库
+ 使用MAP/RED并发处理
+ Mysql数据库的并发访问瓶颈
+ MAP/RED设计技巧——setup()
+ 两类输出结果（增强日志，待爬清单）
+ MAP/REDUCE 自定义OutputFormat

###实例分类
>+ 实例库设计：使用KV内存数据库Flare/Redis
+ 使用MAP/RED并发处理
+ 需要实时更新（setup函数不适用）

##内容识别
>+ 爬虫模块——`Nutch`（权限验证，防封策略，动态代理，动态改变agent）
+ 网页信息清洗、整理（标签补全，格式化，特定信息抽取）
+ 主题分类——自然语言处理（分词，模型训练），PLSA模型

##BI统计分析
>+ 云ETL——HIVE
+ Python脚本
+ HIVE任务调度
+ 业务模型
+ 数据入库

##报表展现查询
>+ Mysql cluster
+ Spring MVC
+ AJAX
+ 数据可视化组件

###系统功能界面：全景分析-用户
![](/pic/hadoop/xm-05.png)
>偏好分析主菜单
>全景分析
>     对访问内容偏好情况的总体分析
>综合分析
>    对内容进行的35个类别的统计
>阅读、新闻、游戏、音乐、视频
>    对五种内容的类型的进一步分析；目前音乐和视频无法分析。
>个性化分析
>     针对某个用户号码的个性化偏好，以及适合推荐的业务。     

>一级标签、二级标签
目前的分类体系方法，仍在优化过程中。

###系统功能界面：综合分析
![](/pic/hadoop/xm-06.png)
![](/pic/hadoop/xm-07.png)


##模块实战
###数据预处理
>+ 需求、设计
+ 技术难点、要点（原子性，上传效率，失败重传、记录）
+ 涉及到的Hadoop相关知识（HDFS）的复习
+ 实战HDFS代码开发及部署运行

###规则库生成
>+ 需求、设计
+ 技术难点、要点（TOP K算法的mapreduce实现）
+ 涉及到的Hadoop相关知识（MAPRED）的复习
+ 实战mapreduce代码开发及部署运行

##需求、设计
>+ 需求：从样本数据中提取有代表性的url
+ 设计（流程）：
    + 读入日志数据
    + 根据url访问的流量进行排序
    + 输出流量占总流量前80%的url
    + 将url列表文本数据导入mysql表

###技术要点
>+ 用mapreduce实现排序
+ 用mapreduce实现topk
+ 将hdfs数据导入mysql

###内容增强
>+ 需求、设计
+ 技术难点、要点（如何实现高效查询外部数据）
+ 涉及到的Hadoop相关知识（MAPRED）的复习
+ 实战mapreduce代码开发及部署运行

>+ 需求：对原始日志进行分类信息增强
+ 设计（流程）：
    + 读入日志数据
    + 过滤脏数据
    + 抽取url字段，查询规则分类表
    + 添加到原始日志
    + 根据分类情况输出两类结果

##技术难点、要点
>Mapreduce中访问外部资源
Mapreduce中如何克服外部资源访问瓶颈
自定义OutputFormat类
